# -*- coding: utf-8 -*-
"""Attention_groundtruth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oSdERXOXEtTcpDDx4ZxeILDq20n_pqJ9
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import math
import numpy as np
from scipy.special import softmax

def test_error(gt, test):
    MPE_error = np.mean(np.abs((gt - test)/gt))
    print("Mean-percentage error is:{}".format(MPE_error))
    return MPE_error


class Attention():
    def __init__(self, d_model, weight_q, weight_k, weight_v, CORDIC_value, CORDIC_shift):
        super().__init__()

        self.d_model = d_model
        self.d_k = d_model
        
        self.weight_q = weight_q
        self.weight_k = weight_k
        self.weight_v = weight_v

        self.CORDIC_value = CORDIC_value
        self.CORDIC_shift = CORDIC_shift
        self.CORDIC_size = len(self.CORDIC_value)


    def forward(self, seq_in):
        v = np.matmul(seq_in, self.weight_v)

        q = np.matmul(seq_in, self.weight_q)

        k = np.matmul(seq_in, self.weight_k)
        
        scores = np.matmul(q, np.transpose(k))/math.sqrt(self.d_k)
        scores = softmax(scores, axis=-1)

        output = np.matmul(scores, v)

        return output


    def my_forward(self, seq_in):
        v = np.matmul(seq_in, self.weight_v)
        q = np.matmul(seq_in, self.weight_q)
        k = np.matmul(seq_in, self.weight_k)

        scores = np.matmul(q, np.transpose(k))/math.sqrt(self.d_k)
        
        scores = self.my_softmax(scores)

        output = np.matmul(scores, v)
        return output


    def my_softmax(self, x):
        numerator = self.my_exp(x)
        denominator = np.sum(numerator, axis=-1, keepdims=True)
        softmax = numerator / denominator
        return softmax

    def my_exp(self, x):
        y = np.ones(x.shape)

        temp_sqrt = x
        temp_fact = 1
        for i in range(2):
            y = y + temp_sqrt/temp_fact

            temp_fact = temp_fact*(i+1)
            temp_sqrt = temp_sqrt*x
        
        return y

np.random.seed(seed=123)


N = 512
d_model = 16

scaling = 2e-1
input = np.random.randn(N,d_model)*scaling
weight_V = np.random.randn(d_model, d_model)*scaling
weight_Q = np.random.randn(d_model, d_model)*scaling
weight_K = np.random.randn(d_model, d_model)*scaling

attention_test = Attention(d_model=d_model, weight_q=weight_Q, weight_k=weight_K, weight_v=weight_V, CORDIC_value=CORDIC_value, CORDIC_shift=CORDIC_shift)

ground_truth = attention_test.forward(input)
print(ground_truth)
test_results = attention_test.my_forward(input)

test_error(ground_truth, test_results);

print(ground_truth)

print(test_results)

np.savetxt('input{}.input.dat'.format(N), input)
np.savetxt('input{}.weight_V.dat'.format(N), weight_V)
np.savetxt('input{}.weight_Q.dat'.format(N), weight_Q)
np.savetxt('input{}.weight_K.dat'.format(N), weight_K)
np.savetxt('output{}.output.dat'.format(N), ground_truth)